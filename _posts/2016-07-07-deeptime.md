---
layout: post
title: Deep Learning for Time Series, pt. 1
description: ""
modified: 2016-07-09
tags: [machine,learning,deep,lstm,recurrent,convolutional,time,series]
comments: true
published: true
image:
  feature: 
  credit: 
  creditlink: 
---

<section id="table-of-contents" class="toc">
  <header>
    <h3>Contents</h3>
  </header>
<div id="drawer" markdown="1">
*  Auto generated table of contents
{:toc}
</div>
</section><!-- /#table-of-contents -->

Time series data occur almost everywhere.  Much of the recent hype (mostly deserved) around deep learning architectures has been around the state-of-the-art performance produced in the fields of computer vision, natural language processing, and bioinformatics.  Much of "deep learning" is a rebranding of neural networks.  During my second year of graduate school, I became interested in applying machine learning to time series data which I planned to use as my dissertation topic.  I ended up studying reinforcement learning in social graphs, but I'll save that for another time.  

### Convolutional Neural Networks

>Convolutional Neural Networks (CNN) are biologically-inspired variants of MLPs. From Hubel and Wiesel’s early work on the cat’s visual cortex, we know the visual cortex contains a complex arrangement of cells. These cells are sensitive to small sub-regions of the visual field, called a receptive field. The sub-regions are tiled to cover the entire visual field. These cells act as local filters over the input space and are well-suited to exploit the strong spatially local correlation present in natural images.<sup>[1](#myfootnote1)</sup>

<p>
    <center><img src="/images/Typical_cnn.png"><br>
    <em>Typical CNN architecture</em></center>
</p>
<br>
CNNs may be applied to multivariate time series.  It can be useful to think of a time series as a one-dimensional image.  You don't need to use an image of the time series as input to the CNN!  I've seen several papers which used this approach which seems weird, you just need to use a 1-dimensional convolutional layer instead of the typical 2-D layer.  In the 1D domain, a convolution can be viewed as a filter, capable of removing outliers, filtering the data or acting as a feature detector, defined to respond maximally to specific temporal sequences within the filter length of the convolution.
<br>
<br>

<p>
    <center><img src="/images/1d_convolution_ex"><br>
    <em>Example of 1-D Convolution</em></center>
</p>

### Data

I'm going to use the UCI Machine Learning <a href="https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones">Human Activity Recognition dataset</a> for this example.  The dataset consists of sensor data recorded from 30 subjects performing activities of daily living, specifically, WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING.  The data was collected from 30 individuals who wore a Samsung Galaxy S II on their waist.  The sampling rate was 50Hz and sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window).  If you don't care about any of this and just want an input into your blackbox, we have 128 observations of a 9-dimensional time series with 6 unique classes.


<p>
    <center><img src="/images/raw.png"><br>
    <em>De-noised sensor data from mobile phone</em></center>
</p>


### Example

{% highlight python %}
from keras.models import Sequential
from keras.layers.core import Dense, Dropout, Activation, Flat
from keras.layers.convolutional import Convolution1D, MaxPooling1D

#set parameters 
batch_size = 50
nb_filter = 125 #number of features to learn
filter_length = 10 #number of time steps
hidden_dims = 250
nb_epoch = 50
n_sensors = 9
classes = 6
pool_length =2
subsample_length =2


model = Sequential()

model.add(Convolution1D(nb_filter=125,
                        filter_length=10,
                        subsample_length=subsample_length,
                        border_mode='same',input_shape=(128,9)))
# we use standard max pooling (halving the output of the previous layer):
model.add(MaxPooling1D(pool_length=pool_length))
# We flatten the output of the conv layer,
# so that we can add a vanilla dense layer:
model.add(Flatten())

model.add(Dense(hidden_dims))
model.add(Dense(classes))
model.add(Activation('softmax'))

model.compile(loss='categorical_crossentropy',optimizer='rmsprop')
model.fit(xtrain,ytrain,batch_size=batch_size,nb_epoch=nb_epoch,show_accuracy=True)




{% endhighlight %}


We can look at the distribution and correlations of the extracted features using the following code.

{% highlight python %}
import seaborn as sns
import pandas as pd
from keras import backend as K

def get_activations(model,layer_idx,X-batch):
	get_activations = K.function([model.layers[0].input, K.learning_phase()],model.layers[layer_idx].output,])
	activations = get_activations([X_batch,0])
	return activations

feats=np.squeeze(activations[0])
features=pd.DataFrame(feats[:,0:10])
features['labels']=np.argmax(ytrain,axis=1)
sns.pairplot(features,hue='labels')


{% endhighlight %}

<img src="/images/features.png">

### Results

Test set summary

```
 class   precision    recall  f1-score   support

 0         0.98      0.92      0.95       496
 1         0.95      0.94      0.94       471
 2         0.89      1.00      0.94       420
 3         0.86      0.92      0.89       491
 4         0.96      0.87      0.91       532
 5         1.00      1.00      1.00       537

total      0.94      0.94      0.94      2947

```

Training set summary

```
class   precision   recall  f1-score   support

0         1.00      1.00      1.00      1226
1         1.00      1.00      1.00      1073
2         1.00      1.00      1.00       986
3         0.91      1.00      0.95      1286
4         1.00      0.91      0.95      1374
5         1.00      1.00      1.00      1407

total     0.98      0.98      0.98      7352
```


A 1-layer convolutional + max pooling neural network achieves 94% accuracy with somewhat arbitrary hyperparameters.  Not bad, but we can improve performance by adding additional layers.  In part 2, I will discuss adding recurrent layers in order to better capture the temporal dynamics.

<br>
<br>
<a name="myfootnote1">1</a>: http://deeplearning.net/tutorial/lenet.html
