<!doctype html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if (IE 7)&!(IEMobile)]><html class="no-js lt-ie9 lt-ie8" lang="en"><![endif]-->
<!--[if (IE 8)&!(IEMobile)]><html class="no-js lt-ie9" lang="en"><![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"><!--<![endif]-->
<head>
<meta charset="utf-8">
<title>Deep Learning for Time Series, pt. 1 &#8211; Nullspace</title>
<meta name="description" content="">
<meta name="keywords" content="machine, learning, deep, lstm, recurrent, convolutional, time, series">
<!-- Twitter Cards -->
	
		<meta name="twitter:card" content="summary">
		<meta name="twitter:image" content=
			
				
						"http://localhost:4000/images/"
				
			
		>
	
	<meta name="twitter:title" content="Deep Learning for Time Series, pt. 1">
	<meta name="twitter:description" content="">
	<meta name="twitter:creator" content="@random_walk">


<!-- Open Graph -->
<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Deep Learning for Time Series, pt. 1">
<meta property="og:description" content="">
<meta property="og:url" content="http://localhost:4000/deeptime/">
<meta property="og:site_name" content="Nullspace">





<link rel="canonical" href="http://localhost:4000/deeptime/">
<link href="http://localhost:4000/feed.xml" type="application/atom+xml" rel="alternate" title="Nullspace Feed">


<!-- http://t.co/dKP3o1e -->
<meta name="HandheldFriendly" content="True">
<meta name="MobileOptimized" content="320">
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<!-- Google Webfonts -->
<link href='http://fonts.googleapis.com/css?family=PT+Sans+Narrow:400,700|PT+Serif:400,700,400italic' rel='stylesheet' type='text/css'>
<!-- For all browsers -->
<link rel="stylesheet" href="http://localhost:4000/assets/css/main.min.css">

<meta http-equiv="cleartype" content="on">

<!-- HTML5 Shiv and Media Query Support -->
<!--[if lt IE 9]>
	<script src="http://localhost:4000/assets/js/vendor/html5shiv.min.js"></script>
	<script src="http://localhost:4000/assets/js/vendor/respond.min.js"></script>
<![endif]-->

<!-- Modernizr -->
<script src="http://localhost:4000/assets/js/vendor/modernizr-2.7.1.custom.min.js"></script>

<!-- Icons -->
<!-- 16x16 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
<!-- 32x32 -->
<link rel="shortcut icon" href="http://localhost:4000/favicon.png">
<!-- 57x57 (precomposed) for iPhone 3GS, pre-2011 iPod Touch and older Android devices -->
<link rel="apple-touch-icon-precomposed" href="http://localhost:4000/images/apple-touch-icon-precomposed.png">
<!-- 72x72 (precomposed) for 1st generation iPad, iPad 2 and iPad mini -->
<link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://localhost:4000/images/apple-touch-icon-72x72-precomposed.png">
<!-- 114x114 (precomposed) for iPhone 4, 4S, 5 and post-2011 iPod Touch -->
<link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://localhost:4000/images/apple-touch-icon-114x114-precomposed.png">
<!-- 144x144 (precomposed) for iPad 3rd and 4th generation -->
<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://localhost:4000/images/apple-touch-icon-144x144-precomposed.png">

</head>

<body class="post">

<!--[if lt IE 9]><div class="browser-upgrade alert alert-info">You are using an <strong>outdated</strong> browser. Please <a href="http://browsehappy.com/">upgrade your browser</a> to improve your experience.</div><![endif]-->

<div class="navigation-wrapper">
	<div class="site-name">
		<a href="http://localhost:4000">Nullspace</a>
	</div><!-- /.site-name -->
	<div class="top-navigation">
		<nav role="navigation" id="site-nav" class="nav">
		    <ul>
		        
				<li><a href="http://localhost:4000/about/" >About</a></li>
		        
				<li><a href="http://localhost:4000/posts/" >Posts</a></li>
		        
				<li><a href="http://localhost:4000/links/" >Links</a></li>
		        
		    </ul>
		</nav>
	</div><!-- /.top-navigation -->
</div><!-- /.navigation-wrapper -->



<div id="main" role="main">
  <div class="article-author-side">
    
	<img src="http://localhost:4000/images/wgm3.jpg" class="bio-photo" alt="Graham Mueller bio photo"></a>

<h3>Graham Mueller</h3>
<p>Applied mathematics refugee, PhD economist, interested in time series, machine learning and graph theory.</p>
<a href="http://twitter.com/random_walk" class="author-social" target="_blank"><i class="fa fa-twitter-square"></i> Twitter</a>





<a href="http://github.com/wgmueller1" class="author-social" target="_blank"><i class="fa fa-github"></i> Github</a>






  </div>
  <article>
    <div class="headline-wrap">
      
        <h1><a href="http://localhost:4000/deeptime/" rel="bookmark" title="Deep Learning for Time Series, pt. 1">Deep Learning for Time Series, pt. 1</a></h1>
      
    </div><!--/ .headline-wrap -->
    <div class="article-wrap">
      <section id="table-of-contents" class="toc">
  <header>
    <h3>Contents</h3>
  </header>
<div id="drawer">
<ul id="markdown-toc">
  <li><a href="#convolutional-neural-networks" id="markdown-toc-convolutional-neural-networks">Convolutional Neural Networks</a></li>
  <li><a href="#data" id="markdown-toc-data">Data</a></li>
  <li><a href="#example" id="markdown-toc-example">Example</a></li>
  <li><a href="#what-is-happening-inside-the-network" id="markdown-toc-what-is-happening-inside-the-network">What is happening inside the network?</a></li>
  <li><a href="#results" id="markdown-toc-results">Results</a></li>
</ul>

  </div>
</section>
<!-- /#table-of-contents -->

<p>Time series data occur almost everywhere.  Much of the recent hype (mostly deserved) around deep learning architectures has been around the state-of-the-art performance produced in the fields of computer vision, natural language processing, and bioinformatics.  Much of “deep learning” is a rebranding of neural networks.  During my second year of graduate school, I became interested in applying machine learning to time series data which I planned to use as my dissertation topic.  I ended up studying reinforcement learning in social graphs, but I’ll save that for another time.  In this tutorial, I’ll discuss applying convolutional neural networks to time series data.</p>

<h3 id="convolutional-neural-networks">Convolutional Neural Networks</h3>

<blockquote>
  <p>Convolutional Neural Networks (CNN) are biologically-inspired variants of MLPs. From Hubel and Wiesel’s early work on the cat’s visual cortex, we know the visual cortex contains a complex arrangement of cells. These cells are sensitive to small sub-regions of the visual field, called a receptive field. The sub-regions are tiled to cover the entire visual field. These cells act as local filters over the input space and are well-suited to exploit the strong spatially local correlation present in natural images.<sup><a href="#myfootnote1">1</a></sup></p>
</blockquote>

<p>
    <center><img src="/images/Typical_cnn.png" /><br />
    <em>Typical CNN architecture</em></center>
</p>
<p><br />
CNNs may be applied to multivariate time series.  It can be useful to think of a time series as a one-dimensional image.  You don’t need to use an image of the time series as input to the CNN!  I’ve seen several papers which used this approach which seems weird, you just need to use a 1-dimensional convolutional layer instead of the typical 2-D layer.  In the 1D domain, a convolution can be viewed as a filter, capable of removing outliers, filtering the data or acting as a feature detector, defined to respond maximally to specific temporal sequences within the filter length of the convolution. 
<br />
<br />
To better understand this, consider the definition of a 1-D convolution.</p>

<script type="math/tex; mode=display">y_{k} = \sum_{n = 0}^{N - 1}h_{n}\,\cdot\,x_{k-n}</script>

<p>There are two input signals, <em>x</em> and <em>h</em> and <em>N</em> is the length of <em>h</em>. The output vector is <em>y</em>, <em>h</em> is known as the kernel or convolution filter and <em>x</em> is the input data.  The subscripts denote the <em>n</em>th element of the vector. Here is an example of how the convolution is calculated when <em style="line-height: 1.5em;">h</em> has three elements:</p>
<p><br /></p>
<p>$$<br />\begin{aligned}<br />y_{0} &amp; = h_{0} \cdot x_{0} \\<br />y_{1} &amp; = h_{1} \cdot x_{0} + h_{0} \cdot x_{1} \\<br />y_{2} &amp; = h_{2} \cdot x_{0} + h_{1} \cdot x_{1} + h_{0} \cdot x_{2} \\<br />y_{3} &amp; = h_{2} \cdot x_{1} + h_{1} \cdot x_{2} + h_{0} \cdot x_{3} \\<br />\vdots<br />\end{aligned}<br />$$</p>
<p><br />
The filter [<script type="math/tex">h_2 h_1 h_0</script>] is slid along the elements of <em>x</em> and at each step we multiply the elements and sum them: the result will be the corresponding element of the output <em>y</em> vector.  Here is a concrete example:
<br /></p>
<p>$$<br />\begin{aligned}<br />x &amp; = \left[<br />\begin{array}{c} 1 \, 2 \, 1 \, 3<br />\end{array}<br />\right]<br />\\<br />h &amp; = \left[<br />\begin{array}{c}<br />2 \, 0 \, 1<br />\end{array}<br />\right]<br />\end{aligned}<br />\\<br />\begin{array}{c|cccccccc|r}<br />k &amp; \it{0} &amp; \it{0} &amp; 1 &amp; 2 &amp; 1 &amp; 3 &amp; \it{0} &amp; \it{0} &amp; \it{y_{k}}\\<br />\hline<br />0 &amp; 1 &amp; 0 &amp; 2 &amp; &amp; &amp; &amp; &amp; &amp; 2 \cdot 1 = 2\\<br />1 &amp; &amp; 1 &amp; 0 &amp; 2 &amp; &amp; &amp; &amp; &amp; 2 \cdot 2 = 4\\<br />2 &amp; &amp; &amp; 1 &amp; 0 &amp; 2 &amp; &amp; &amp; &amp; 2 \cdot 1 + 1 \cdot 1 = 3\\<br />3 &amp; &amp; &amp; &amp; 1 &amp; 0 &amp; 2 &amp; &amp; &amp; 2 \cdot 3 + 1 \cdot 2 = 8\\<br />4 &amp; &amp; &amp; &amp; &amp; 1 &amp; 0 &amp; 2 &amp; &amp; 1 \cdot 1 = 1\\<br />5 &amp; &amp; &amp; &amp; &amp; &amp; 1 &amp; 0 &amp; 2 &amp; 1 \cdot 3 = 3\\<br />\end{array}<br />$$</p>
<p><br />
It is easy to see that a simple moving average is a special case of a convolution using the following kernel:<br /></p>

<p>$$<br />h = \left[<br />\begin{array}{c} \dfrac{1}{N} \, \dfrac{1}{N} \, ... \, \dfrac{1}{N}\, \dfrac{1}{N}\end{array}<br />\right]<br />$$</p>
<p><br />
<br /></p>

<p>Hopefully, this gives some intuition as to what a one-dimensional convolution does. A 1-dimensional convolution applied to multiple input signals creates a linear combination of the filtered inputs at each time step.</p>

<h3 id="data">Data</h3>

<p>I’m going to use the UCI Machine Learning <a href="https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones">Human Activity Recognition dataset</a> for this example.  The dataset consists of sensor data recorded from 30 subjects performing activities of daily living, specifically, WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING.  The data was collected from 30 individuals who wore a Samsung Galaxy S II on their waist.  The sampling rate was 50Hz and sensor signals (accelerometer and gyroscope) were pre-processed by applying noise filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap (128 readings/window).  If you don’t care about any of this and just want an input into your blackbox, we have 128 observations of a 9-dimensional time series with 6 unique classes.</p>

<p>
    <center><img src="/images/raw.png" /><br />
    <em>De-noised sensor data from mobile phone</em></center>
</p>

<h3 id="example">Example</h3>

<p>I’m a big fan of the neural networks library <a href="http://keras.io" target="_blank">Keras</a>.  Keras can run on top of the deep learning libraries Theano and Tensorflow and makes prototyping neural networks very easy.  Here I’ll use Keras to build a convolutional neural network to predict the activities from the sensor data provided by Human Activity Recognition dataset.</p>

<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers.core</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Dropout</span><span class="p">,</span> <span class="n">Activation</span><span class="p">,</span> <span class="n">Flat</span>
<span class="kn">from</span> <span class="nn">keras.layers.convolutional</span> <span class="kn">import</span> <span class="n">Convolution1D</span><span class="p">,</span> <span class="n">MaxPooling1D</span>

<span class="c">#set parameters </span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">nb_filter</span> <span class="o">=</span> <span class="mi">64</span> <span class="c">#number of features to learn</span>
<span class="n">filter_length</span> <span class="o">=</span> <span class="mi">10</span> <span class="c">#number of time steps</span>
<span class="n">hidden_dims</span> <span class="o">=</span> <span class="mi">250</span>
<span class="n">nb_epoch</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">n_sensors</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">classes</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">pool_length</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">subsample_length</span> <span class="o">=</span> <span class="mi">2</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Convolution1D</span><span class="p">(</span><span class="n">nb_filter</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span>
                        <span class="n">filter_length</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">subsample_length</span><span class="o">=</span><span class="n">subsample_length</span><span class="p">,</span>
                        <span class="n">border_mode</span><span class="o">=</span><span class="s">'same'</span><span class="p">,</span><span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span><span class="mi">9</span><span class="p">)))</span>
<span class="c"># we use standard max pooling (halving the output of the previous layer):</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">MaxPooling1D</span><span class="p">(</span><span class="n">pool_length</span><span class="o">=</span><span class="n">pool_length</span><span class="p">))</span>
<span class="c"># We flatten the output of the conv layer,</span>
<span class="c"># so that we can add a vanilla dense layer:</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Flatten</span><span class="p">())</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">hidden_dims</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">classes</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Activation</span><span class="p">(</span><span class="s">'softmax'</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span><span class="n">optimizer</span><span class="o">=</span><span class="s">'rmsprop'</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">xtrain</span><span class="p">,</span><span class="n">ytrain</span><span class="p">,</span><span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span><span class="n">nb_epoch</span><span class="o">=</span><span class="n">nb_epoch</span><span class="p">,</span><span class="n">show_accuracy</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span></code></pre></figure>

<!--
After training the model, we can look at the output of each layer using the following code to get a better sense of what is happening inside the neural network.  In this case, the convolutional layer maps the raw input into 64 seperate time series each of length 64 and the max pooling layer condenses these into a single time series of length 64.  



<figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">K</span>

<span class="k">def</span> <span class="nf">get_activations</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="n">layer_idx</span><span class="p">,</span><span class="n">X</span><span class="o">-</span><span class="n">batch</span><span class="p">):</span>
	<span class="n">get_activations</span> <span class="o">=</span> <span class="n">K</span><span class="o">.</span><span class="n">function</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="nb">input</span><span class="p">,</span> <span class="n">K</span><span class="o">.</span><span class="n">learning_phase</span><span class="p">()],</span><span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer_idx</span><span class="p">]</span><span class="o">.</span><span class="n">output</span><span class="p">,])</span>
	<span class="n">activations</span> <span class="o">=</span> <span class="n">get_activations</span><span class="p">([</span><span class="n">X_batch</span><span class="p">,</span><span class="mi">0</span><span class="p">])</span>
	<span class="k">return</span> <span class="n">activations</span>

<span class="c">#the following plots the first 32 activation outputs </span>
<span class="c"># (the middle plot in the figure below)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>

<span class="c">#t = np.arange(0, 1, 0.01)</span>
<span class="n">activations</span> <span class="o">=</span> <span class="n">get_activations</span><span class="p">(</span><span class="n">model</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="n">xtrain</span><span class="p">)</span> <span class="c"># same result as above</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">1</span><span class="p">,:,:]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ax</span><span class="p">):</span>
    <span class="n">a</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="p">,:]</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">a</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span><span class="mi">128</span><span class="p">])</span>
    <span class="n">a</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="nb">min</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="p">,:]),</span> <span class="n">np</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">activations</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="n">i</span><span class="p">,:])])</span>
    <span class="n">a</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">'off'</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></code></pre></figure>


<img src="/images/cnn_filters.png">
-->

<h3 id="what-is-happening-inside-the-network">What is happening inside the network?</h3>

<p>Here are a few examples of some well known filters from digital signal processing:<br /></p>

<p><img src="http://www.dspguide.com/graphics/F_6_3.gif" /><br /></p>

<p>High and low pass filters remove certain frequencies from the original signal.  You can think of the high-pass filter as removing any underlying trend in the signal and the low-pass filter as smoothing the signal to extract the trend.</p>

<p><img src="http://www.dspguide.com/graphics/F_6_4.gif" /><br /></p>

<p>The derivative of an input signal can be approximated by convolving the derivative of a Gaussian Filter or by using a discrete derivative filter (as shown above).  Since we typically use a max pooling layer after a convolutional layer, we are able to identify the maximum value of the derivative over the time interval of the input.  Inverting attenuators flip the signal and enable us to extract minimum values of the signal’s derivative using a max pooling layer. 
<br /></p>

<p>After training the model, we can look at some of the learned filters to get a sense of what types of features the CNN extracts.</p>

<div class="container" style="inline-block">
    <figure>
    <img src="/images/learned_derivative.png" width="300" />
    <img src="/images/learned_high_pass.png" width="300" />
    </figure>   
</div>

<p>The filter on the left is learned for the y-axis acceleration and looks very close to a derivative filter.  The filter on the right is similar to a high-pass filter and was learned for the g1 gyroscope axis.  Some of the other filters aren’t as obvious, however, it appears that many of the learned filters are similar to well known techniques from signal processing.</p>

<div class="container" style="inline-block">
    <figure>
    <img src="/images/filters.png" width="300" />
    <img src="/images/filters2.png" width="300" />
    </figure>   
</div>

<p>Images from <a href="http://www.dspguide.com">http://www.dspguide.com</a></p>

<h3 id="results">Results</h3>

<p>Test set summary:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class   precision    recall  f1-score   support
0         0.98      0.92      0.95       496
1         0.95      0.94      0.94       471
2         0.89      1.00      0.94       420
3         0.86      0.92      0.89       491
4         0.96      0.87      0.91       532
5         1.00      1.00      1.00       537

total      0.94      0.94      0.94      2947
</code></pre></div></div>

<p>Training set summary:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class   precision   recall  f1-score   support

0         1.00      1.00      1.00      1226
1         1.00      1.00      1.00      1073
2         1.00      1.00      1.00       986
3         0.91      1.00      0.95      1286
4         1.00      0.91      0.95      1374
5         1.00      1.00      1.00      1407

total     0.98      0.98      0.98      7352
</code></pre></div></div>

<p>A 1-layer convolutional + max pooling neural network achieves 94% accuracy with somewhat arbitrary hyperparameters (I choose the parameters to help simplify the visualizations of each of the activations.) This is a good start, but we can improve performance by adding additional layers.  In part 2, I will discuss adding recurrent layers in order to better capture the temporal dynamics.</p>

<p><br />
<br />
<a name="myfootnote1">1</a>: http://deeplearning.net/tutorial/lenet.html</p>

      <hr />
      <footer role="contentinfo">
        <div class="article-author-bottom">
          
	<img src="http://localhost:4000/images/wgm3.jpg" class="bio-photo" alt="Graham Mueller bio photo"></a>

<h3>Graham Mueller</h3>
<p>Applied mathematics refugee, PhD economist, interested in time series, machine learning and graph theory.</p>
<a href="http://twitter.com/random_walk" class="author-social" target="_blank"><i class="fa fa-twitter-square"></i> Twitter</a>





<a href="http://github.com/wgmueller1" class="author-social" target="_blank"><i class="fa fa-github"></i> Github</a>






        </div>
        <p class="byline"><strong>Deep Learning for Time Series, pt. 1</strong> was published on <time datetime="2016-07-07T00:00:00-04:00">July 07, 2016</time> and last modified on <time datetime="2016-07-09">July 09, 2016</time> by <a href="http://localhost:4000/about" title="About Graham Mueller">Graham Mueller</a>.</p>
      </footer>
    </div><!-- /.article-wrap -->
  
  </article>
</div><!-- /#main -->

<div class="footer-wrap">
  <div class="related-articles">
  <h4>You might also enjoy <small class="pull-right">(<a href="http://localhost:4000/posts/">View all posts</a>)</small></h4>
    <ul>
    
      <li><a href="http://localhost:4000/hebbian/" title="Hebbian Learning">Hebbian Learning</a></li>
    
      <li><a href="http://localhost:4000/boxing/" title="Boxing's Grandmasters Part II">Boxing's Grandmasters Part II</a></li>
    
      <li><a href="http://localhost:4000/grandmaster/" title="Boxing's Grandmasters">Boxing's Grandmasters</a></li>
    
    </ul>
    <hr />
  </div><!-- /.related-articles -->
  <footer>
    <span>&copy; 2018 Graham Mueller. Powered by <a href="http://jekyllrb.com">Jekyll</a> using the <a href="http://mademistakes.com/minimal-mistakes/">Minimal Mistakes</a> theme.</span>

  </footer>
</div><!-- /.footer-wrap -->

<script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
<script type="text/javascript"
    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script>window.jQuery || document.write('<script src="http://localhost:4000/assets/js/vendor/jquery-1.9.1.min.js"><\/script>')</script>
<script src="http://localhost:4000/assets/js/scripts.min.js"></script>


  
	        

</body>
</html>